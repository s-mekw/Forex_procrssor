"""
å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ã“ã®ãƒ†ã‚¹ãƒˆã§ã¯ä»¥ä¸‹ã‚’è¦–è¦šçš„ã«ç¢ºèªã§ãã¾ã™ï¼š
- å¤§é‡ã®å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
- Polarsã®lazyãƒ•ãƒ¬ãƒ¼ãƒ æ©Ÿèƒ½ã«ã‚ˆã‚‹é…å»¶è©•ä¾¡
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒãƒ£ãƒ³ã‚¯å‡¦ç†
- å‹•çš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã¨ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
- å‡¦ç†é€²è¡ŒçŠ¶æ³ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®å¯è¦–åŒ–
"""

import sys
import asyncio
import time
import gc
from pathlib import Path
from datetime import datetime, timedelta, UTC
from typing import List, Dict, Any, Optional, Iterator
import polars as pl
import psutil
from rich.console import Console
from rich.live import Live
from rich.layout import Layout
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn, SpinnerColumn
from rich.text import Text
from rich import box
from dataclasses import dataclass

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.mt5_data_acquisition.mt5_client import MT5ConnectionManager
from src.mt5_data_acquisition.ohlc_fetcher import OHLCFetcher
from src.data_processing.processor import PolarsProcessingEngine, MemoryLimitError
from src.common.models import OHLC
from src.common.config import get_config, ConfigManager
from demo_config import get_demo_config

console = Console()

@dataclass
class StreamingStats:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†çµ±è¨ˆ"""
    total_records_processed: int = 0
    total_chunks_processed: int = 0
    current_chunk_size: int = 0
    memory_usage_mb: float = 0.0
    processing_rate_per_sec: float = 0.0
    average_chunk_time: float = 0.0
    memory_optimization_savings: float = 0.0
    gc_collections: int = 0
    lazy_optimizations_applied: int = 0
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []

class LargeDatasetStreaming:
    """å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.console = Console()
        self.connection_manager = None
        self.ohlc_fetcher = None
        self.polars_engine = None
        self.stats = StreamingStats()
        # ãƒ‡ãƒ¢è¨­å®šã‚’èª­ã¿è¾¼ã¿
        self.demo_config = get_demo_config()
        
        # å‡¦ç†è¨­å®šï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
        self.symbols = self.demo_config.get_symbols_list('streaming')
        self.timeframe = "H1"  # 1æ™‚é–“è¶³
        self.days_back = 30    # 30æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
        self.chunk_times = []
        self.memory_snapshots = []
        self.processing_start_time = None
        
    def setup_connections(self) -> bool:
        """æ¥ç¶šã¨ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–"""
        try:
            # ConfigManagerã‹ã‚‰è¨­å®šã‚’å–å¾—
            config_manager = ConfigManager()
            config_manager.load_config()
            config = get_config()
            
            # MT5æ¥ç¶šç”¨ã®è¨­å®šè¾æ›¸ã‚’ä½œæˆ
            mt5_config = {
                "account": int(config.mt5_login),  # intå‹ã«å¤‰æ›
                "password": config.mt5_password.get_secret_value() if config.mt5_password else "",  # SecretStrã‹ã‚‰å€¤ã‚’å–å¾—
                "server": str(config.mt5_server),
                "timeout": int(config.mt5_timeout),
                "path": config.mt5_path  # MT5å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
            }
            
            self.connection_manager = MT5ConnectionManager()
            if not self.connection_manager.connect(mt5_config):
                console.print("[red]âŒ MT5ã¸ã®æ¥ç¶šã«å¤±æ•—[/red]")
                return False
                
            self.ohlc_fetcher = OHLCFetcher(self.connection_manager)
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨Polarsã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆè¨­å®šã‹ã‚‰å–å¾—ï¼‰
            polars_config = self.demo_config.get_polars_engine_config('streaming')
            self.polars_engine = PolarsProcessingEngine(
                chunk_size=polars_config.chunk_size
            )
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä¿å­˜
            self.streaming_batch_size = polars_config.streaming_batch_size
            
            console.print("[green]âœ… å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†[/green]")
            console.print(f"[cyan]ğŸ“Š é€šè²¨ãƒšã‚¢: {', '.join(self.symbols)}[/cyan]")
            console.print(f"[cyan]âš™ï¸ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.streaming_batch_size}[/cyan]")
            return True
            
        except Exception as e:
            self.stats.errors.append(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            console.print(f"[red]âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}[/red]")
            return False
    
    def get_memory_metrics(self) -> Dict[str, float]:
        """ãƒ¡ãƒ¢ãƒªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            'rss_mb': memory_info.rss / (1024 * 1024),
            'vms_mb': memory_info.vms / (1024 * 1024),
            'available_mb': psutil.virtual_memory().available / (1024 * 1024),
            'percent': process.memory_percent()
        }
    
    async def fetch_historical_data_stream(self) -> Iterator[List[OHLC]]:
        """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å½¢å¼ã§å–å¾—"""
        end_date = datetime.now(UTC)
        start_date = end_date - timedelta(days=self.days_back)
        
        console.print(f"[yellow]ğŸ“ˆ {len(self.symbols)}éŠ˜æŸ„ã®{self.days_back}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—é–‹å§‹[/yellow]")
        
        for symbol in self.symbols:
            try:
                # ã‚·ãƒ³ãƒœãƒ«ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                console.print(f"[cyan]å–å¾—ä¸­: {symbol} ({self.timeframe})[/cyan]")
                
                ohlc_data = await asyncio.to_thread(
                    self.ohlc_fetcher.fetch_ohlc_range,
                    symbol=symbol,
                    timeframe=self.timeframe,
                    start_time=start_date,
                    end_time=end_date
                )
                
                if ohlc_data:
                    # ãƒ‡ãƒ¼ã‚¿ã‚’å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
                    chunk_size = 50  # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
                    for i in range(0, len(ohlc_data), chunk_size):
                        chunk = ohlc_data[i:i + chunk_size]
                        yield chunk
                        await asyncio.sleep(0.01)  # å°‘ã—å¾…æ©Ÿã—ã¦CPUè² è·ã‚’è»½æ¸›
                        
            except Exception as e:
                error_msg = f"{symbol}ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}"
                self.stats.errors.append(error_msg)
                console.print(f"[red]âš ï¸ {error_msg}[/red]")
    
    def create_dataframe_from_ohlc(self, ohlc_list: List[OHLC]) -> pl.DataFrame:
        """OHLCãƒªã‚¹ãƒˆã‹ã‚‰Polarsãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ"""
        if not ohlc_list:
            return pl.DataFrame()
            
        data = []
        for ohlc in ohlc_list:
            data.append({
                'timestamp': ohlc.timestamp,
                'symbol': ohlc.symbol,
                'open': float(ohlc.open),
                'high': float(ohlc.high),
                'low': float(ohlc.low),
                'close': float(ohlc.close),
                'volume': ohlc.volume,
                'spread': float(ohlc.high) - float(ohlc.low),
                'body': abs(float(ohlc.close) - float(ohlc.open)),
                'direction': 1 if float(ohlc.close) > float(ohlc.open) else -1
            })
        
        return pl.DataFrame(data)
    
    def create_lazy_processing_pipeline(self, df: pl.DataFrame) -> pl.LazyFrame:
        """é…å»¶è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ"""
        self.stats.lazy_optimizations_applied += 1
        
        return (
            df.lazy()
            .with_columns([
                # ç§»å‹•å¹³å‡è¨ˆç®—
                pl.col("close").rolling_mean(window_size=5).alias("sma_5"),
                pl.col("close").rolling_mean(window_size=20).alias("sma_20"),
                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™
                pl.col("spread").rolling_std(window_size=10).alias("spread_volatility"),
                # ä¾¡æ ¼å¤‰åŒ–ç‡
                pl.col("close").pct_change().alias("price_change_pct")
            ])
            .with_columns([
                # ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ
                (pl.col("sma_5") > pl.col("sma_20")).alias("bullish_signal"),
                # ç•°å¸¸å€¤æ¤œå‡º
                (pl.col("spread") > pl.col("spread").quantile(0.95)).alias("high_spread")
            ])
            .filter(
                pl.col("volume") > 0  # ãƒœãƒªãƒ¥ãƒ¼ãƒ 0ã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
            )
        )
    
    async def process_streaming_chunks(self):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        self.processing_start_time = time.time()
        
        # é€²è¡ŒçŠ¶æ³è¿½è·¡ç”¨
        total_expected_chunks = len(self.symbols) * (self.days_back * 24 // 50)  # æ¦‚ç®—
        processed_chunks = 0
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
            console=console
        ) as progress:
            
            task = progress.add_task("å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­...", total=total_expected_chunks)
            
            # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®ãƒ©ã‚¤ãƒ–æ›´æ–°
            with Live(console=console, refresh_per_second=3) as live:
                
                async for data_chunk in self.fetch_historical_data_stream():
                    chunk_start_time = time.time()
                    
                    try:
                        # DataFrameã‚’ä½œæˆ
                        df = self.create_dataframe_from_ohlc(data_chunk)
                        
                        if not df.is_empty():
                            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                            memory_before = self.get_memory_metrics()['rss_mb']
                            
                            # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
                            optimized_df = self.polars_engine.optimize_dtypes(df)
                            
                            # é…å»¶è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ
                            lazy_pipeline = self.create_lazy_processing_pipeline(optimized_df)
                            
                            # ãƒ¡ãƒ¢ãƒªåˆ¶é™ãƒã‚§ãƒƒã‚¯ã¨èª¿æ•´
                            try:
                                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œï¼ˆé…å»¶è©•ä¾¡ã‚’å®Ÿéš›ã«è©•ä¾¡ï¼‰
                                result_df = lazy_pipeline.collect()
                                
                                # é›†ç´„å‡¦ç†
                                summary = result_df.group_by("symbol").agg([
                                    pl.col("close").mean().alias("avg_close"),
                                    pl.col("volume").sum().alias("total_volume"),
                                    pl.col("bullish_signal").sum().alias("bullish_count"),
                                    pl.col("high_spread").sum().alias("high_spread_count"),
                                    pl.col("price_change_pct").std().alias("volatility")
                                ])
                                
                                # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–åŠ¹æœã‚’è¨ˆç®—
                                memory_after = self.get_memory_metrics()['rss_mb']
                                memory_saved = memory_before - memory_after
                                if memory_saved > 0:
                                    self.stats.memory_optimization_savings += memory_saved
                                
                            except MemoryLimitError:
                                # ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«å¼•ã£ã‹ã‹ã£ãŸå ´åˆã®å¯¾å‡¦
                                console.print("[yellow]âš ï¸ ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’èª¿æ•´ä¸­...[/yellow]")
                                
                                # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å‹•çš„ã«èª¿æ•´
                                self.polars_engine.adjust_chunk_size(-20)  # 20æ¸›ã‚‰ã™
                                
                                # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
                                gc.collect()
                                self.stats.gc_collections += 1
                                
                                # å†åº¦å‡¦ç†ã‚’è©¦è¡Œï¼ˆç°¡æ˜“ç‰ˆï¼‰
                                simple_summary = optimized_df.group_by("symbol").agg([
                                    pl.col("close").mean().alias("avg_close"),
                                    pl.col("volume").sum().alias("total_volume")
                                ])
                            
                            # çµ±è¨ˆæƒ…å ±æ›´æ–°
                            chunk_time = time.time() - chunk_start_time
                            self.chunk_times.append(chunk_time)
                            self.stats.total_records_processed += len(data_chunk)
                            self.stats.total_chunks_processed += 1
                            self.stats.current_chunk_size = len(data_chunk)
                            self.stats.memory_usage_mb = self.get_memory_metrics()['rss_mb']
                            
                            # å‡¦ç†é€Ÿåº¦è¨ˆç®—
                            elapsed_time = time.time() - self.processing_start_time
                            self.stats.processing_rate_per_sec = self.stats.total_records_processed / max(elapsed_time, 1)
                            self.stats.average_chunk_time = sum(self.chunk_times) / len(self.chunk_times)
                            
                            # ãƒ¡ãƒ¢ãƒªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
                            self.memory_snapshots.append(self.stats.memory_usage_mb)
                        
                    except Exception as e:
                        error_msg = f"ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)[:100]}"
                        self.stats.errors.append(error_msg)
                        console.print(f"[red]âŒ {error_msg}[/red]")
                    
                    # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
                    processed_chunks += 1
                    progress.update(task, advance=1, 
                                  description=f"å‡¦ç†æ¸ˆã¿: {self.stats.total_records_processed:,}ãƒ¬ã‚³ãƒ¼ãƒ‰")
                    
                    # ãƒ©ã‚¤ãƒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°
                    live.update(self.create_streaming_dashboard())
                    
                    # ãƒ¡ãƒ¢ãƒªåœ§è¿«æ™‚ã®èª¿æ•´
                    if self.stats.memory_usage_mb > 200:  # 200MBä»¥ä¸Šã®å ´åˆ
                        gc.collect()
                        self.stats.gc_collections += 1
                        await asyncio.sleep(0.1)  # å°‘ã—å¾…æ©Ÿ
    
    def create_streaming_dashboard(self) -> Layout:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"""
        layout = Layout()
        
        layout.split_column(
            Layout(name="header", size=3),
            Layout(name="main"),
            Layout(name="performance", size=10)
        )
        
        layout["main"].split_row(
            Layout(name="stats"),
            Layout(name="memory")
        )
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        header_text = Text("ğŸ“Š å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†", style="bold green")
        layout["header"].update(Panel(header_text, box=box.ROUNDED))
        
        # å·¦å´ï¼šå‡¦ç†çµ±è¨ˆ
        stats_table = Table(title="å‡¦ç†çµ±è¨ˆ", box=box.SIMPLE)
        stats_table.add_column("ãƒ¡ãƒˆãƒªã‚¯ã‚¹", style="cyan")
        stats_table.add_column("å€¤", style="green")
        
        stats_table.add_row("å‡¦ç†æ¸ˆã¿ãƒ¬ã‚³ãƒ¼ãƒ‰", f"{self.stats.total_records_processed:,}")
        stats_table.add_row("å‡¦ç†æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯", f"{self.stats.total_chunks_processed:,}")
        stats_table.add_row("ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", str(self.stats.current_chunk_size))
        stats_table.add_row("å‡¦ç†é€Ÿåº¦", f"{self.stats.processing_rate_per_sec:.1f} ãƒ¬ã‚³ãƒ¼ãƒ‰/ç§’")
        stats_table.add_row("å¹³å‡ãƒãƒ£ãƒ³ã‚¯å‡¦ç†æ™‚é–“", f"{self.stats.average_chunk_time:.3f}ç§’")
        stats_table.add_row("é…å»¶è©•ä¾¡æœ€é©åŒ–", str(self.stats.lazy_optimizations_applied))
        
        layout["stats"].update(Panel(stats_table, title="ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çµ±è¨ˆ", box=box.ROUNDED))
        
        # å³å´ï¼šãƒ¡ãƒ¢ãƒªç®¡ç†
        memory_table = Table(title="ãƒ¡ãƒ¢ãƒªç®¡ç†", box=box.SIMPLE)
        memory_table.add_column("é …ç›®", style="cyan")
        memory_table.add_column("å€¤", style="yellow")
        
        memory_table.add_row("ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡", f"{self.stats.memory_usage_mb:.2f}MB")
        memory_table.add_row("ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å‰Šæ¸›é‡", f"{self.stats.memory_optimization_savings:.2f}MB")
        memory_table.add_row("GCå®Ÿè¡Œå›æ•°", str(self.stats.gc_collections))
        
        # ãƒ¡ãƒ¢ãƒªå‚¾å‘
        if len(self.memory_snapshots) >= 5:
            recent_trend = sum(self.memory_snapshots[-5:]) / 5
            older_trend = sum(self.memory_snapshots[:5]) / min(5, len(self.memory_snapshots))
            trend = "ğŸ“ˆ" if recent_trend > older_trend else "ğŸ“‰" if recent_trend < older_trend else "â¡ï¸"
            memory_table.add_row("ãƒ¡ãƒ¢ãƒªä½¿ç”¨å‚¾å‘", trend)
        
        # åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª
        available_memory = self.get_memory_metrics()['available_mb']
        memory_table.add_row("åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª", f"{available_memory:.2f}MB")
        
        layout["memory"].update(Panel(memory_table, title="ãƒ¡ãƒ¢ãƒªç›£è¦–", box=box.ROUNDED))
        
        # ä¸‹éƒ¨ï¼šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©³ç´°
        perf_table = Table(title="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©³ç´°", box=box.SIMPLE)
        perf_table.add_column("é …ç›®", style="cyan")
        perf_table.add_column("å€¤", style="green")
        perf_table.add_column("çŠ¶æ…‹", style="yellow")
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåŠ¹ç‡æ€§
        current_chunk_size = self.polars_engine.chunk_size
        perf_table.add_row("å‹•çš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", str(current_chunk_size), 
                          "âš¡" if current_chunk_size > 50 else "ğŸŒ")
        
        # å‡¦ç†åŠ¹ç‡
        if self.chunk_times:
            recent_avg = sum(self.chunk_times[-5:]) / min(5, len(self.chunk_times))
            efficiency = "ğŸš€" if recent_avg < 0.1 else "âš¡" if recent_avg < 0.5 else "ğŸŒ"
            perf_table.add_row("å‡¦ç†åŠ¹ç‡", f"{recent_avg:.3f}s/chunk", efficiency)
        
        # ã‚¨ãƒ©ãƒ¼çŠ¶æ³
        error_count = len(self.stats.errors)
        error_status = "âœ…" if error_count == 0 else "âš ï¸" if error_count < 5 else "âŒ"
        perf_table.add_row("ã‚¨ãƒ©ãƒ¼æ•°", str(error_count), error_status)
        
        # ã‚·ã‚¹ãƒ†ãƒ è² è·
        cpu_percent = psutil.cpu_percent()
        cpu_status = "ğŸ”¥" if cpu_percent > 80 else "âš¡" if cpu_percent > 50 else "ğŸ˜´"
        perf_table.add_row("CPUä½¿ç”¨ç‡", f"{cpu_percent:.1f}%", cpu_status)
        
        layout["performance"].update(Panel(perf_table, title="ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹", box=box.ROUNDED))
        
        return layout
    
    def print_final_report(self):
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›"""
        console.print("\n" + "="*60)
        console.print("[bold green]ğŸ‰ å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ[/bold green]")
        console.print("="*60)
        
        # å‡¦ç†ã‚µãƒãƒªãƒ¼
        total_time = time.time() - self.processing_start_time if self.processing_start_time else 0
        
        summary_table = Table(title="å‡¦ç†ã‚µãƒãƒªãƒ¼", box=box.ROUNDED)
        summary_table.add_column("é …ç›®", style="cyan")
        summary_table.add_column("å€¤", style="green")
        
        summary_table.add_row("ç·å‡¦ç†æ™‚é–“", f"{total_time:.2f}ç§’")
        summary_table.add_row("å‡¦ç†ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", f"{self.stats.total_records_processed:,}")
        summary_table.add_row("å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°", f"{self.stats.total_chunks_processed:,}")
        summary_table.add_row("å¹³å‡å‡¦ç†é€Ÿåº¦", f"{self.stats.processing_rate_per_sec:.1f} ãƒ¬ã‚³ãƒ¼ãƒ‰/ç§’")
        summary_table.add_row("ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å‰Šæ¸›é‡", f"{self.stats.memory_optimization_savings:.2f}MB")
        summary_table.add_row("é…å»¶è©•ä¾¡æœ€é©åŒ–å®Ÿè¡Œå›æ•°", str(self.stats.lazy_optimizations_applied))
        summary_table.add_row("ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œå›æ•°", str(self.stats.gc_collections))
        
        console.print(summary_table)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        if self.chunk_times:
            console.print("\n[bold]ğŸ“ˆ å‡¦ç†æ™‚é–“çµ±è¨ˆ:[/bold]")
            avg_time = sum(self.chunk_times) / len(self.chunk_times)
            min_time = min(self.chunk_times)
            max_time = max(self.chunk_times)
            
            perf_stats_table = Table(box=box.SIMPLE)
            perf_stats_table.add_column("çµ±è¨ˆ", style="cyan")
            perf_stats_table.add_column("æ™‚é–“", style="green")
            
            perf_stats_table.add_row("å¹³å‡å‡¦ç†æ™‚é–“", f"{avg_time:.4f}ç§’")
            perf_stats_table.add_row("æœ€çŸ­å‡¦ç†æ™‚é–“", f"{min_time:.4f}ç§’")
            perf_stats_table.add_row("æœ€é•·å‡¦ç†æ™‚é–“", f"{max_time:.4f}ç§’")
            
            console.print(perf_stats_table)
        
        # ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
        if self.stats.errors:
            console.print(f"\n[red]âš ï¸ ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼ ({len(self.stats.errors)}ä»¶):[/red]")
            for i, error in enumerate(self.stats.errors[-10:], 1):  # æœ€æ–°10ä»¶ã®ã¿
                console.print(f"  {i}. {error}")
        else:
            console.print("\n[green]âœ… ã‚¨ãƒ©ãƒ¼ãªã—ã§å®Œäº†ã—ã¾ã—ãŸ[/green]")
    
    async def run_large_dataset_demo(self):
        """å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ãƒ¢ã®å®Ÿè¡Œ"""
        console.print("[bold blue]ğŸ“Š å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ãƒ‡ãƒ¢ã‚’é–‹å§‹ã—ã¾ã™[/bold blue]\n")
        
        if not self.setup_connections():
            return
        
        try:
            await self.process_streaming_chunks()
            
        except KeyboardInterrupt:
            console.print("\n[yellow]âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­[/yellow]")
            
        except Exception as e:
            console.print(f"[red]âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}[/red]")
            self.stats.errors.append(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            
        finally:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.connection_manager and self.connection_manager.is_connected():
                self.connection_manager.disconnect()
            
            # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
            self.print_final_report()

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    demo = LargeDatasetStreaming()
    await demo.run_large_dataset_demo()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        console.print("\n[yellow]ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ[/yellow]")
    except Exception as e:
        console.print(f"[red]å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}[/red]")