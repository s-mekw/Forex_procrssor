"""
ãƒ‡ãƒ¼ã‚¿é›†ç´„å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ã“ã®ãƒ†ã‚¹ãƒˆã§ã¯ä»¥ä¸‹ã‚’è¦–è¦šçš„ã«ç¢ºèªã§ãã¾ã™ï¼š
- è¤‡æ•°ã®æ™‚é–“è»¸ã§ã®ãƒ‡ãƒ¼ã‚¿é›†ç´„å‡¦ç†
- è¤‡é›‘ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ã‚°ãƒ«ãƒ¼ãƒ—åŒ–å‡¦ç†
- ã‚«ã‚¹ã‚¿ãƒ é›†ç´„é–¢æ•°ã®é©ç”¨
- å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å¯è¦–åŒ–
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†
"""

import sys
import asyncio
import time
from pathlib import Path
from datetime import datetime, timedelta, UTC
from typing import List, Dict, Any, Optional, Tuple
from enum import Enum
import polars as pl
import psutil
from rich.console import Console
from rich.live import Live
from rich.layout import Layout
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn
from rich.text import Text
from rich import box
from dataclasses import dataclass

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.mt5_data_acquisition.mt5_client import MT5ConnectionManager
from src.mt5_data_acquisition.tick_fetcher import TickDataStreamer, StreamerConfig
from src.data_processing.processor import PolarsProcessingEngine
from src.common.models import Tick
from src.common.config import get_config, ConfigManager

console = Console()

class TimeFrame(Enum):
    """æ™‚é–“è»¸å®šç¾©"""
    TICK = "tick"
    SECOND_1 = "1s"
    SECOND_5 = "5s"
    SECOND_30 = "30s"
    MINUTE_1 = "1m"
    MINUTE_5 = "5m"
    MINUTE_15 = "15m"

@dataclass
class PipelineStage:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ®µéšã®å®šç¾©"""
    name: str
    description: str
    execution_time: float = 0.0
    input_rows: int = 0
    output_rows: int = 0
    memory_usage_mb: float = 0.0
    completed: bool = False

class DataAggregationPipeline:
    """ãƒ‡ãƒ¼ã‚¿é›†ç´„å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.console = Console()
        self.connection_manager = None
        self.tick_streamer = None
        self.polars_engine = None
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ®µéš
        self.pipeline_stages = [
            PipelineStage("data_collection", "ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿åé›†"),
            PipelineStage("data_cleaning", "ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æ¤œè¨¼"),
            PipelineStage("multi_timeframe_agg", "â° è¤‡æ•°æ™‚é–“è»¸é›†ç´„"),
            PipelineStage("technical_indicators", "ğŸ“ˆ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—"),
            PipelineStage("statistical_analysis", "ğŸ“Š çµ±è¨ˆåˆ†æ"),
            PipelineStage("anomaly_detection", "ğŸ” ç•°å¸¸å€¤æ¤œå‡º"),
            PipelineStage("final_aggregation", "ğŸ¯ æœ€çµ‚é›†ç´„ãƒ»å‡ºåŠ›")
        ]
        
        # åé›†ãƒ‡ãƒ¼ã‚¿
        self.raw_ticks: List[Tick] = []
        self.processed_data: Dict[str, pl.DataFrame] = {}
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
        self.start_time = None
        self.stage_times = {}
        
    def setup_connections(self) -> bool:
        """æ¥ç¶šã¨ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–"""
        try:
            # ConfigManagerã‹ã‚‰è¨­å®šã‚’å–å¾—
            config_manager = ConfigManager()
            config_manager.load_config()
            config = get_config()
            
            # MT5æ¥ç¶šç”¨ã®è¨­å®šè¾æ›¸ã‚’ä½œæˆ
            mt5_config = {
                "account": int(config.mt5_login),  # intå‹ã«å¤‰æ›
                "password": str(config.mt5_password),  # strå‹ã‚’æ˜ç¤º
                "server": str(config.mt5_server),
                "timeout": int(config.mt5_timeout)
            }
            
            self.connection_manager = MT5ConnectionManager()
            if not self.connection_manager.connect(mt5_config):
                console.print("[red]âŒ MT5ã¸ã®æ¥ç¶šã«å¤±æ•—[/red]")
                return False
                
            # ãƒ†ã‚£ãƒƒã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒãƒ¼è¨­å®š
            config = StreamerConfig(
                symbol="EURUSD",
                buffer_size=2000,
                spike_threshold_percent=0.2,
                backpressure_threshold=0.7
            )
            self.tick_streamer = TickDataStreamer(
                connection_manager=self.connection_manager,
                config=config
            )
            
            # Polarsã‚¨ãƒ³ã‚¸ãƒ³
            self.polars_engine = PolarsProcessingEngine(
                max_memory_mb=200,
                chunk_size=200
            )
            
            console.print("[green]âœ… ãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†[/green]")
            return True
            
        except Exception as e:
            console.print(f"[red]âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}[/red]")
            return False
    
    def get_memory_usage(self) -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰"""
        process = psutil.Process()
        return process.memory_info().rss / (1024 * 1024)
    
    async def stage_1_data_collection(self) -> None:
        """æ®µéš1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿åé›†"""
        stage = self.pipeline_stages[0]
        stage_start = time.time()
        
        console.print("[yellow]ğŸ“Š æ®µéš1: ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹ã—ã¾ã™...[/yellow]")
        
        await self.tick_streamer.subscribe_to_ticks()
        
        # 60ç§’é–“ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        collection_duration = 60
        with Progress(
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            console=console
        ) as progress:
            task = progress.add_task("ãƒ†ã‚£ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...", total=collection_duration)
            
            for i in range(collection_duration):
                await asyncio.sleep(1)
                
                # æ–°ã—ã„ãƒ†ã‚£ãƒƒã‚¯ã‚’å–å¾—
                new_ticks = self.tick_streamer.get_new_ticks()
                self.raw_ticks.extend(new_ticks)
                
                progress.update(task, advance=1, 
                              description=f"åé›†æ¸ˆã¿: {len(self.raw_ticks)}ãƒ†ã‚£ãƒƒã‚¯")
        
        await self.tick_streamer.unsubscribe()
        
        # æ®µéšå®Œäº†
        stage.execution_time = time.time() - stage_start
        stage.input_rows = 0
        stage.output_rows = len(self.raw_ticks)
        stage.memory_usage_mb = self.get_memory_usage()
        stage.completed = True
        
        console.print(f"[green]âœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(self.raw_ticks)}ãƒ†ã‚£ãƒƒã‚¯[/green]")
    
    def stage_2_data_cleaning(self) -> None:
        """æ®µéš2: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»æ¤œè¨¼"""
        stage = self.pipeline_stages[1]
        stage_start = time.time()
        stage.input_rows = len(self.raw_ticks)
        
        console.print("[yellow]ğŸ§¹ æ®µéš2: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...[/yellow]")
        
        if not self.raw_ticks:
            console.print("[red]âš ï¸ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“[/red]")
            return
        
        # DataFrameã‚’ä½œæˆ
        data = []
        for tick in self.raw_ticks:
            data.append({
                'timestamp': tick.timestamp,
                'symbol': tick.symbol,
                'bid': float(tick.bid),
                'ask': float(tick.ask),
                'volume': tick.volume,
                'spread': float(tick.ask) - float(tick.bid)
            })
        
        df = pl.DataFrame(data)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        cleaned_df = (
            df
            .filter(pl.col("bid") > 0)  # ç„¡åŠ¹ãªbidä¾¡æ ¼ã‚’é™¤å¤–
            .filter(pl.col("ask") > 0)  # ç„¡åŠ¹ãªaskä¾¡æ ¼ã‚’é™¤å¤–
            .filter(pl.col("spread") >= 0)  # è² ã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚’é™¤å¤–
            .filter(pl.col("spread") < pl.col("bid") * 0.01)  # ç•°å¸¸ã«å¤§ããªã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚’é™¤å¤–
            .filter(pl.col("volume") >= 0)  # è² ã®ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’é™¤å¤–
            .with_columns([
                # ä¸­å€¤è¨ˆç®—
                ((pl.col("bid") + pl.col("ask")) / 2).alias("mid_price"),
                # ä¾¡æ ¼å¤‰åŒ–ç‡
                pl.col("bid").pct_change().alias("bid_change_pct"),
                pl.col("ask").pct_change().alias("ask_change_pct")
            ])
            .drop_nulls()  # nullå€¤ã‚’é™¤å¤–
            .sort("timestamp")  # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
        )
        
        # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
        cleaned_df = self.polars_engine.optimize_dtypes(cleaned_df)
        self.processed_data["cleaned"] = cleaned_df
        
        stage.execution_time = time.time() - stage_start
        stage.output_rows = cleaned_df.height
        stage.memory_usage_mb = self.get_memory_usage()
        stage.completed = True
        
        console.print(f"[green]âœ… ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {stage.input_rows} â†’ {stage.output_rows}è¡Œ[/green]")
    
    def stage_3_multi_timeframe_aggregation(self) -> None:
        """æ®µéš3: è¤‡æ•°æ™‚é–“è»¸é›†ç´„"""
        stage = self.pipeline_stages[2]
        stage_start = time.time()
        
        if "cleaned" not in self.processed_data:
            console.print("[red]âš ï¸ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“[/red]")
            return
        
        df = self.processed_data["cleaned"]
        stage.input_rows = df.height
        
        console.print("[yellow]â° æ®µéš3: è¤‡æ•°æ™‚é–“è»¸é›†ç´„ã‚’å®Ÿè¡Œä¸­...[/yellow]")
        
        # 1ç§’é–“éš”ã®é›†ç´„
        df_1s = (
            df
            .with_columns(
                pl.col("timestamp").dt.truncate("1s").alias("time_bucket")
            )
            .group_by("time_bucket")
            .agg([
                pl.col("bid").first().alias("open_bid"),
                pl.col("bid").max().alias("high_bid"),
                pl.col("bid").min().alias("low_bid"),
                pl.col("bid").last().alias("close_bid"),
                pl.col("ask").first().alias("open_ask"),
                pl.col("ask").max().alias("high_ask"),
                pl.col("ask").min().alias("low_ask"),
                pl.col("ask").last().alias("close_ask"),
                pl.col("volume").sum().alias("total_volume"),
                pl.col("spread").mean().alias("avg_spread"),
                pl.len().alias("tick_count")
            ])
            .sort("time_bucket")
        )
        
        # 5ç§’é–“éš”ã®é›†ç´„
        df_5s = (
            df
            .with_columns(
                pl.col("timestamp").dt.truncate("5s").alias("time_bucket")
            )
            .group_by("time_bucket")
            .agg([
                pl.col("mid_price").first().alias("open"),
                pl.col("mid_price").max().alias("high"),
                pl.col("mid_price").min().alias("low"),
                pl.col("mid_price").last().alias("close"),
                pl.col("volume").sum().alias("volume"),
                pl.col("spread").mean().alias("avg_spread"),
                pl.col("spread").std().alias("spread_volatility"),
                pl.len().alias("tick_count")
            ])
            .sort("time_bucket")
        )
        
        # 30ç§’é–“éš”ã®é›†ç´„
        df_30s = (
            df
            .with_columns(
                pl.col("timestamp").dt.truncate("30s").alias("time_bucket")
            )
            .group_by("time_bucket")
            .agg([
                pl.col("mid_price").first().alias("open"),
                pl.col("mid_price").max().alias("high"),
                pl.col("mid_price").min().alias("low"),
                pl.col("mid_price").last().alias("close"),
                pl.col("volume").sum().alias("volume"),
                pl.col("spread").mean().alias("avg_spread"),
                pl.col("bid_change_pct").std().alias("price_volatility"),
                pl.len().alias("tick_count")
            ])
            .sort("time_bucket")
        )
        
        self.processed_data["agg_1s"] = df_1s
        self.processed_data["agg_5s"] = df_5s
        self.processed_data["agg_30s"] = df_30s
        
        total_output_rows = df_1s.height + df_5s.height + df_30s.height
        
        stage.execution_time = time.time() - stage_start
        stage.output_rows = total_output_rows
        stage.memory_usage_mb = self.get_memory_usage()
        stage.completed = True
        
        console.print(f"[green]âœ… æ™‚é–“è»¸é›†ç´„å®Œäº†: 1s({df_1s.height}), 5s({df_5s.height}), 30s({df_30s.height})[/green]")
    
    def stage_4_technical_indicators(self) -> None:
        """æ®µéš4: ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—"""
        stage = self.pipeline_stages[3]
        stage_start = time.time()
        
        if "agg_5s" not in self.processed_data:
            console.print("[red]âš ï¸ é›†ç´„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“[/red]")
            return
        
        df = self.processed_data["agg_5s"]
        stage.input_rows = df.height
        
        console.print("[yellow]ğŸ“ˆ æ®µéš4: ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...[/yellow]")
        
        # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ã‚’è¿½åŠ 
        df_with_indicators = df.with_columns([
            # ç§»å‹•å¹³å‡
            pl.col("close").rolling_mean(window_size=5).alias("sma_5"),
            pl.col("close").rolling_mean(window_size=10).alias("sma_10"),
            pl.col("close").rolling_mean(window_size=20).alias("sma_20"),
            
            # æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆè¿‘ä¼¼ï¼‰
            pl.col("close").ewm_mean(half_life=5).alias("ema_5"),
            pl.col("close").ewm_mean(half_life=10).alias("ema_10"),
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™
            pl.col("close").rolling_std(window_size=10).alias("volatility_10"),
            pl.col("high").rolling_max(window_size=10).alias("highest_10"),
            pl.col("low").rolling_min(window_size=10).alias("lowest_10"),
            
            # RSIè¿‘ä¼¼ï¼ˆä¾¡æ ¼å¤‰åŒ–ãƒ™ãƒ¼ã‚¹ï¼‰
            pl.col("close").pct_change().alias("price_change"),
        ]).with_columns([
            # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰è¿‘ä¼¼
            (pl.col("sma_20") + 2 * pl.col("volatility_10")).alias("bb_upper"),
            (pl.col("sma_20") - 2 * pl.col("volatility_10")).alias("bb_lower"),
            
            # %Kï¼ˆã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹è¿‘ä¼¼ï¼‰
            ((pl.col("close") - pl.col("lowest_10")) / 
             (pl.col("highest_10") - pl.col("lowest_10")) * 100).alias("stoch_k"),
        ]).with_columns([
            # %Dï¼ˆ%Kã®ç§»å‹•å¹³å‡ï¼‰
            pl.col("stoch_k").rolling_mean(window_size=3).alias("stoch_d"),
            
            # ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ
            (pl.col("close") > pl.col("sma_20")).alias("bullish_ma"),
            (pl.col("ema_5") > pl.col("ema_10")).alias("bullish_ema"),
            (pl.col("close") > pl.col("bb_upper")).alias("overbought"),
            (pl.col("close") < pl.col("bb_lower")).alias("oversold"),
        ])
        
        self.processed_data["technical"] = df_with_indicators
        
        stage.execution_time = time.time() - stage_start
        stage.output_rows = df_with_indicators.height
        stage.memory_usage_mb = self.get_memory_usage()
        stage.completed = True
        
        console.print(f"[green]âœ… ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™è¨ˆç®—å®Œäº†: {stage.output_rows}è¡Œ[/green]")
    
    def stage_5_statistical_analysis(self) -> None:
        """æ®µéš5: çµ±è¨ˆåˆ†æ"""
        stage = self.pipeline_stages[4]
        stage_start = time.time()
        
        if "technical" not in self.processed_data:
            console.print("[red]âš ï¸ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“[/red]")
            return
        
        df = self.processed_data["technical"]
        stage.input_rows = df.height
        
        console.print("[yellow]ğŸ“Š æ®µéš5: çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œä¸­...[/yellow]")
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼ä½œæˆ
        stats_summary = df.select([
            pl.col("close").mean().alias("avg_close"),
            pl.col("close").std().alias("std_close"),
            pl.col("close").min().alias("min_close"),
            pl.col("close").max().alias("max_close"),
            pl.col("close").quantile(0.25).alias("q25_close"),
            pl.col("close").quantile(0.75).alias("q75_close"),
            pl.col("volume").sum().alias("total_volume"),
            pl.col("volume").mean().alias("avg_volume"),
            pl.col("avg_spread").mean().alias("overall_avg_spread"),
            pl.col("volatility_10").mean().alias("avg_volatility"),
            pl.col("bullish_ma").sum().alias("bullish_ma_count"),
            pl.col("bullish_ema").sum().alias("bullish_ema_count"),
            pl.col("overbought").sum().alias("overbought_count"),
            pl.col("oversold").sum().alias("oversold_count"),
            pl.len().alias("total_periods")
        ])
        
        # ç›¸é–¢åˆ†æ
        correlation_data = df.select([
            "close", "volume", "avg_spread", "volatility_10", "stoch_k"
        ]).drop_nulls()
        
        if correlation_data.height > 1:
            # Polarsã§ç›¸é–¢ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            corr_analysis = correlation_data.with_columns([
                pl.corr("close", "volume").alias("close_volume_corr"),
                pl.corr("close", "avg_spread").alias("close_spread_corr"),
                pl.corr("volume", "volatility_10").alias("volume_volatility_corr")
            ]).select([
                "close_volume_corr", "close_spread_corr", "volume_volatility_corr"
            ]).head(1)
        
        # æ™‚ç³»åˆ—åˆ†æ
        trend_analysis = df.with_columns([
            pl.col("close").pct_change().alias("returns"),
        ]).filter(pl.col("returns").is_not_null()).with_columns([
            (pl.col("returns") > 0).alias("positive_return"),
        ]).select([
            pl.col("returns").mean().alias("avg_return"),
            pl.col("returns").std().alias("return_volatility"),
            pl.col("positive_return").sum().alias("positive_periods"),
            pl.len().alias("total_return_periods")
        ])
        
        self.processed_data["stats_summary"] = stats_summary
        if 'corr_analysis' in locals():
            self.processed_data["correlation"] = corr_analysis
        self.processed_data["trend_analysis"] = trend_analysis
        
        stage.execution_time = time.time() - stage_start
        stage.output_rows = stats_summary.height + trend_analysis.height
        stage.memory_usage_mb = self.get_memory_usage()
        stage.completed = True
        
        console.print(f"[green]âœ… çµ±è¨ˆåˆ†æå®Œäº†[/green]")
    
    def stage_6_anomaly_detection(self) -> None:
        """æ®µéš6: ç•°å¸¸å€¤æ¤œå‡º"""
        stage = self.pipeline_stages[5]
        stage_start = time.time()
        
        if "technical" not in self.processed_data:
            console.print("[red]âš ï¸ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“[/red]")
            return
        
        df = self.processed_data["technical"]
        stage.input_rows = df.height
        
        console.print("[yellow]ğŸ” æ®µéš6: ç•°å¸¸å€¤æ¤œå‡ºã‚’å®Ÿè¡Œä¸­...[/yellow]")
        
        # ç•°å¸¸å€¤æ¤œå‡º
        df_with_anomalies = df.with_columns([
            # Z-score based anomaly detection
            ((pl.col("close") - pl.col("close").mean()) / pl.col("close").std()).abs().alias("close_zscore"),
            ((pl.col("volume") - pl.col("volume").mean()) / pl.col("volume").std()).abs().alias("volume_zscore"),
            ((pl.col("avg_spread") - pl.col("avg_spread").mean()) / pl.col("avg_spread").std()).abs().alias("spread_zscore"),
        ]).with_columns([
            # ç•°å¸¸å€¤ãƒ•ãƒ©ã‚°
            (pl.col("close_zscore") > 3).alias("price_anomaly"),
            (pl.col("volume_zscore") > 3).alias("volume_anomaly"),
            (pl.col("spread_zscore") > 3).alias("spread_anomaly"),
            
            # IQR based anomaly detection for volatility
            (pl.col("volatility_10") > pl.col("volatility_10").quantile(0.75) + 1.5 * 
             (pl.col("volatility_10").quantile(0.75) - pl.col("volatility_10").quantile(0.25))).alias("volatility_anomaly"),
        ]).with_columns([
            # è¤‡åˆç•°å¸¸å€¤
            (pl.col("price_anomaly") | pl.col("volume_anomaly") | 
             pl.col("spread_anomaly") | pl.col("volatility_anomaly")).alias("any_anomaly")
        ])
        
        # ç•°å¸¸å€¤ã‚µãƒãƒªãƒ¼
        anomaly_summary = df_with_anomalies.select([
            pl.col("price_anomaly").sum().alias("price_anomalies"),
            pl.col("volume_anomaly").sum().alias("volume_anomalies"),
            pl.col("spread_anomaly").sum().alias("spread_anomalies"),
            pl.col("volatility_anomaly").sum().alias("volatility_anomalies"),
            pl.col("any_anomaly").sum().alias("total_anomalies"),
            pl.len().alias("total_records"),
            (pl.col("any_anomaly").sum() / pl.len() * 100).alias("anomaly_rate_percent")
        ])
        
        self.processed_data["anomalies"] = df_with_anomalies
        self.processed_data["anomaly_summary"] = anomaly_summary
        
        stage.execution_time = time.time() - stage_start
        stage.output_rows = df_with_anomalies.height
        stage.memory_usage_mb = self.get_memory_usage()
        stage.completed = True
        
        total_anomalies = anomaly_summary.select("total_anomalies").item()
        console.print(f"[green]âœ… ç•°å¸¸å€¤æ¤œå‡ºå®Œäº†: {total_anomalies}ä»¶ã®ç•°å¸¸å€¤ã‚’æ¤œå‡º[/green]")
    
    def stage_7_final_aggregation(self) -> None:
        """æ®µéš7: æœ€çµ‚é›†ç´„ãƒ»å‡ºåŠ›"""
        stage = self.pipeline_stages[6]
        stage_start = time.time()
        
        console.print("[yellow]ğŸ¯ æ®µéš7: æœ€çµ‚é›†ç´„ãƒ»å‡ºåŠ›ã‚’å®Ÿè¡Œä¸­...[/yellow]")
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
        final_report = {}
        
        if "stats_summary" in self.processed_data:
            stats = self.processed_data["stats_summary"].row(0)
            final_report["statistics"] = {
                "avg_price": float(stats[0]),
                "price_volatility": float(stats[1]),
                "price_range": float(stats[3]) - float(stats[2]),
                "total_volume": float(stats[6]),
                "avg_spread": float(stats[8])
            }
        
        if "anomaly_summary" in self.processed_data:
            anomalies = self.processed_data["anomaly_summary"].row(0)
            final_report["anomalies"] = {
                "total_anomalies": int(anomalies[4]),
                "anomaly_rate": float(anomalies[6]),
                "price_anomalies": int(anomalies[0]),
                "volume_anomalies": int(anomalies[1])
            }
        
        if "trend_analysis" in self.processed_data:
            trend = self.processed_data["trend_analysis"].row(0)
            final_report["trends"] = {
                "avg_return": float(trend[0]) if trend[0] is not None else 0.0,
                "return_volatility": float(trend[1]) if trend[1] is not None else 0.0,
                "positive_rate": float(trend[2]) / float(trend[3]) * 100 if trend[3] > 0 else 0.0
            }
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        total_processing_time = sum(s.execution_time for s in self.pipeline_stages if s.completed)
        total_input_rows = self.pipeline_stages[0].output_rows
        
        final_report["performance"] = {
            "total_processing_time": total_processing_time,
            "total_input_rows": total_input_rows,
            "processing_rate": total_input_rows / total_processing_time if total_processing_time > 0 else 0,
            "peak_memory_mb": max(s.memory_usage_mb for s in self.pipeline_stages if s.completed),
            "stages_completed": sum(1 for s in self.pipeline_stages if s.completed)
        }
        
        self.processed_data["final_report"] = final_report
        
        stage.execution_time = time.time() - stage_start
        stage.input_rows = sum(s.output_rows for s in self.pipeline_stages[:-1] if s.completed)
        stage.output_rows = 1  # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        stage.memory_usage_mb = self.get_memory_usage()
        stage.completed = True
        
        console.print("[green]âœ… æœ€çµ‚é›†ç´„å®Œäº†[/green]")
    
    def create_pipeline_dashboard(self) -> Layout:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"""
        layout = Layout()
        
        layout.split_column(
            Layout(name="header", size=3),
            Layout(name="main"),
            Layout(name="details", size=12)
        )
        
        layout["main"].split_row(
            Layout(name="pipeline"),
            Layout(name="metrics")
        )
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        header_text = Text("ğŸ”„ ãƒ‡ãƒ¼ã‚¿é›†ç´„å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³", style="bold magenta")
        layout["header"].update(Panel(header_text, box=box.ROUNDED))
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é€²è¡ŒçŠ¶æ³
        pipeline_table = Table(title="ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ®µéš", box=box.SIMPLE)
        pipeline_table.add_column("æ®µéš", style="cyan")
        pipeline_table.add_column("çŠ¶æ…‹", style="green")
        pipeline_table.add_column("å®Ÿè¡Œæ™‚é–“", style="yellow")
        pipeline_table.add_column("å‡¦ç†è¡Œæ•°", style="blue")
        
        for stage in self.pipeline_stages:
            status = "âœ…" if stage.completed else "â³" if stage.execution_time > 0 else "â¸ï¸"
            execution_time = f"{stage.execution_time:.3f}s" if stage.execution_time > 0 else "-"
            row_info = f"{stage.input_rows} â†’ {stage.output_rows}" if stage.completed else "-"
            
            pipeline_table.add_row(stage.description, status, execution_time, row_info)
        
        layout["pipeline"].update(Panel(pipeline_table, title="å‡¦ç†é€²è¡ŒçŠ¶æ³", box=box.ROUNDED))
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        metrics_table = Table(title="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹", box=box.SIMPLE)
        metrics_table.add_column("é …ç›®", style="cyan")
        metrics_table.add_column("å€¤", style="green")
        
        completed_stages = [s for s in self.pipeline_stages if s.completed]
        if completed_stages:
            total_time = sum(s.execution_time for s in completed_stages)
            total_input = self.pipeline_stages[0].output_rows if self.pipeline_stages[0].completed else 0
            current_memory = self.get_memory_usage()
            
            metrics_table.add_row("å®Œäº†æ®µéš", f"{len(completed_stages)}/{len(self.pipeline_stages)}")
            metrics_table.add_row("ç·å‡¦ç†æ™‚é–“", f"{total_time:.3f}s")
            metrics_table.add_row("å‡¦ç†é€Ÿåº¦", f"{total_input/total_time:.1f} è¡Œ/ç§’" if total_time > 0 else "-")
            metrics_table.add_row("ç¾åœ¨ãƒ¡ãƒ¢ãƒª", f"{current_memory:.2f}MB")
            metrics_table.add_row("ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª", f"{max(s.memory_usage_mb for s in completed_stages):.2f}MB" if completed_stages else "-")
        else:
            metrics_table.add_row("å‡¦ç†çŠ¶æ…‹", "å¾…æ©Ÿä¸­")
        
        layout["metrics"].update(Panel(metrics_table, title="ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹", box=box.ROUNDED))
        
        # è©³ç´°çµæœ
        if "final_report" in self.processed_data:
            report = self.processed_data["final_report"]
            
            results_table = Table(title="å‡¦ç†çµæœã‚µãƒãƒªãƒ¼", box=box.SIMPLE)
            results_table.add_column("ã‚«ãƒ†ã‚´ãƒª", style="cyan")
            results_table.add_column("é …ç›®", style="yellow")
            results_table.add_column("å€¤", style="green")
            
            if "statistics" in report:
                stats = report["statistics"]
                results_table.add_row("çµ±è¨ˆ", "å¹³å‡ä¾¡æ ¼", f"{stats['avg_price']:.5f}")
                results_table.add_row("", "ä¾¡æ ¼å¤‰å‹•å¹…", f"{stats['price_range']:.5f}")
                results_table.add_row("", "ç·ãƒœãƒªãƒ¥ãƒ¼ãƒ ", f"{stats['total_volume']:,.0f}")
                results_table.add_row("", "å¹³å‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰", f"{stats['avg_spread']:.5f}")
            
            if "anomalies" in report:
                anomalies = report["anomalies"]
                results_table.add_row("ç•°å¸¸å€¤", "æ¤œå‡ºæ•°", str(anomalies["total_anomalies"]))
                results_table.add_row("", "ç•°å¸¸å€¤ç‡", f"{anomalies['anomaly_rate']:.2f}%")
            
            if "trends" in report:
                trends = report["trends"]
                results_table.add_row("ãƒˆãƒ¬ãƒ³ãƒ‰", "å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³", f"{trends['avg_return']:.6f}")
                results_table.add_row("", "ä¸Šæ˜‡æœŸé–“ç‡", f"{trends['positive_rate']:.1f}%")
            
            layout["details"].update(Panel(results_table, title="æœ€çµ‚çµæœ", box=box.ROUNDED))
        else:
            layout["details"].update(Panel("å‡¦ç†çµæœå¾…æ©Ÿä¸­...", title="æœ€çµ‚çµæœ", box=box.ROUNDED))
        
        return layout
    
    def print_final_summary(self):
        """æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›"""
        console.print("\n" + "="*70)
        console.print("[bold green]ğŸ‰ ãƒ‡ãƒ¼ã‚¿é›†ç´„å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ[/bold green]")
        console.print("="*70)
        
        if "final_report" not in self.processed_data:
            console.print("[red]æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“[/red]")
            return
        
        report = self.processed_data["final_report"]
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
        if "performance" in report:
            perf = report["performance"]
            console.print(f"\n[bold cyan]ğŸ“Š å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:[/bold cyan]")
            console.print(f"â€¢ ç·å‡¦ç†æ™‚é–“: [yellow]{perf['total_processing_time']:.3f}ç§’[/yellow]")
            console.print(f"â€¢ å‡¦ç†è¡Œæ•°: [yellow]{perf['total_input_rows']:,}è¡Œ[/yellow]")
            console.print(f"â€¢ å‡¦ç†é€Ÿåº¦: [green]{perf['processing_rate']:.1f}è¡Œ/ç§’[/green]")
            console.print(f"â€¢ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: [yellow]{perf['peak_memory_mb']:.2f}MB[/yellow]")
            console.print(f"â€¢ å®Œäº†æ®µéš: [green]{perf['stages_completed']}/{len(self.pipeline_stages)}[/green]")
        
        # çµ±è¨ˆæƒ…å ±
        if "statistics" in report:
            stats = report["statistics"]
            console.print(f"\n[bold cyan]ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:[/bold cyan]")
            console.print(f"â€¢ å¹³å‡ä¾¡æ ¼: [yellow]{stats['avg_price']:.5f}[/yellow]")
            console.print(f"â€¢ ä¾¡æ ¼å¤‰å‹•å¹…: [yellow]{stats['price_range']:.5f}[/yellow]")
            console.print(f"â€¢ ç·ãƒœãƒªãƒ¥ãƒ¼ãƒ : [yellow]{stats['total_volume']:,.0f}[/yellow]")
            console.print(f"â€¢ å¹³å‡ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰: [yellow]{stats['avg_spread']:.5f}[/yellow]")
        
        # ç•°å¸¸å€¤æƒ…å ±
        if "anomalies" in report:
            anomalies = report["anomalies"]
            console.print(f"\n[bold cyan]ğŸ” ç•°å¸¸å€¤æ¤œå‡º:[/bold cyan]")
            console.print(f"â€¢ ç·ç•°å¸¸å€¤æ•°: [red]{anomalies['total_anomalies']}[/red]")
            console.print(f"â€¢ ç•°å¸¸å€¤ç‡: [red]{anomalies['anomaly_rate']:.2f}%[/red]")
            console.print(f"â€¢ ä¾¡æ ¼ç•°å¸¸: [yellow]{anomalies['price_anomalies']}[/yellow]")
            console.print(f"â€¢ ãƒœãƒªãƒ¥ãƒ¼ãƒ ç•°å¸¸: [yellow]{anomalies['volume_anomalies']}[/yellow]")
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±
        if "trends" in report:
            trends = report["trends"]
            console.print(f"\n[bold cyan]ğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ:[/bold cyan]")
            console.print(f"â€¢ å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³: [green]{trends['avg_return']:.6f}[/green]")
            console.print(f"â€¢ ãƒªã‚¿ãƒ¼ãƒ³å¤‰å‹•: [yellow]{trends['return_volatility']:.6f}[/yellow]")
            console.print(f"â€¢ ä¸Šæ˜‡æœŸé–“ç‡: [green]{trends['positive_rate']:.1f}%[/green]")
        
        console.print("\n[green]âœ… å…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ®µéšãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ[/green]")
    
    async def run_aggregation_pipeline(self):
        """é›†ç´„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        console.print("[bold blue]ğŸ”„ ãƒ‡ãƒ¼ã‚¿é›†ç´„å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹ã—ã¾ã™[/bold blue]\n")
        
        if not self.setup_connections():
            return
        
        self.start_time = time.time()
        
        try:
            with Live(self.create_pipeline_dashboard(), console=console, refresh_per_second=2) as live:
                # å„æ®µéšã‚’é †æ¬¡å®Ÿè¡Œ
                await self.stage_1_data_collection()
                live.update(self.create_pipeline_dashboard())
                
                self.stage_2_data_cleaning()
                live.update(self.create_pipeline_dashboard())
                
                self.stage_3_multi_timeframe_aggregation()
                live.update(self.create_pipeline_dashboard())
                
                self.stage_4_technical_indicators()
                live.update(self.create_pipeline_dashboard())
                
                self.stage_5_statistical_analysis()
                live.update(self.create_pipeline_dashboard())
                
                self.stage_6_anomaly_detection()
                live.update(self.create_pipeline_dashboard())
                
                self.stage_7_final_aggregation()
                live.update(self.create_pipeline_dashboard())
                
                # æœ€çµ‚ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’10ç§’é–“è¡¨ç¤º
                await asyncio.sleep(10)
            
        except KeyboardInterrupt:
            console.print("\n[yellow]âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­[/yellow]")
            
        except Exception as e:
            console.print(f"[red]âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}[/red]")
            
        finally:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.connection_manager and self.connection_manager.is_connected():
                self.connection_manager.disconnect()
            
            # æœ€çµ‚ã‚µãƒãƒªãƒ¼
            self.print_final_summary()

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    pipeline = DataAggregationPipeline()
    await pipeline.run_aggregation_pipeline()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        console.print("\n[yellow]ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ[/yellow]")
    except Exception as e:
        console.print(f"[red]å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}[/red]")